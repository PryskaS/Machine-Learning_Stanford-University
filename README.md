# ~~~~~~~~~~~~ Machine Learning Course ~~~~~~~~~
<p align="center">
  <img width="500" height="130" src="https://github.com/PryskaS/Machine-Learning_Stanford-University/blob/master/images/coursera-logo.png">
</p> 

<p align="center">
  <img width="234" height="70" src="https://github.com/PryskaS/Machine-Learning_Stanford-University/blob/master/images/stanford-logo.png">
</p> 


<p align="center"> <b><i>PROJECT IN PROGRESS</b></i>
  </p> 

Syllabus
----------------------------------------------------------
### [Week 1](https://github.com/PryskaS/Machine-Learning_Stanford-University/tree/master/01.%20Week)
----------------------------------------------------------
- **Introduction**
  - [x] What is Machine Learning?
  - [x] Supervised Learning
  - [x] Unsupervised Learning
  
  > [1 practice exercise]

- **Linear Regression with One Variable**
  - [x] Model Representation
  - [x] Cost Function
  - [x] Gradient Descent
  - [x] Gradient Descent For Linear Regression
  
  > 1. [1 practice exercise] 
  > 1. [Linear Regression I using Python]
  > 1. [Cost Function using Python]
  > 1. [Gradient Descent using Python]
  
- **Linear Algebra Review**
  - [ ] Matrices and Vectors
  - [ ] Addition and Scalar Multiplication
  - [ ] Matrix Vector Multiplication
  - [ ] Matrix Matrix Multiplication
  - [ ] Matrix Multiplication Properties
  - [ ] Inverse and Transpose
  
  > 1. [1 practice exercise]
  > 1. [Python Programming]
  
  ### [Week 2](https://github.com/PryskaS/Machine-Learning_Stanford-University/tree/master/02.%20Week)
----------------------------------------------------------
- **Linear Regression with Multiple Variables**
  - [ ] Multiple Features
  - [ ] Gradient Descent for Multiple Variables
  - [ ] Gradient Descent in Practice I - Feature Scaling
  - [ ] Gradient Descent in Practice II - Learning Rate
  - [ ] Features and Polynomial Regression
  - [ ] Normal Equation
  - [ ] Normal Equation Noninvertibility
  - [ ] Working on and Submitting Programming Assignments
  
  > [1 practice exercise]
  
- **Octave/Matlab Tutorial**
  - [ ] Basic Operations
  - [ ] Moving Data Around
  - [ ] Computing on Data
  - [ ] Plotting Data
  - [ ] Control Statements: for, while, if statement
  - [ ] Vectorization
  
  > [1 practice exercise]
  
  ### [Week 3](https://github.com/PryskaS/Machine-Learning_Stanford-University/tree/master/03.%20Week)
----------------------------------------------------------
- **Logistic Regression**
  - [ ] Classification
  - [ ] Hypothesis Representation
  - [ ] Decision Boundary
  - [ ] Cost Function
  - [ ] Simplified Cost Function and Gradient Descent
  - [ ] Advanced Optimization
  - [ ] Multiclass Classification: One-vs-all
  
  > [1 practice exercise]
  
- **Regularization**
  - [ ] The Problem of Overfitting
  - [ ] Cost Function
  - [ ] Regularized Linear Regression
  - [ ] Regularized Logistic Regression
 
  > [1 practice exercise]
 
  ### [Week 4](https://github.com/PryskaS/Machine-Learning_Stanford-University/tree/master/04.%20Week)
----------------------------------------------------------
- **Neural Networks: Representation**
  - [ ] Non-linear Hypotheses
  - [ ] Neurons and the Brain
  - [ ] Model Representation I
  - [ ] Model Representation II
  - [ ] Examples and Intuitions I
  - [ ] Examples and Intuitions II
  - [ ] Multiclass Classification

  > [1 practice exercise]
  
  ### [Week 5](https://github.com/PryskaS/Machine-Learning_Stanford-University/tree/master/05.%20Week)
----------------------------------------------------------
- **Neural Networks: Learning**
  - [ ] Cost Function
  - [ ] Backpropagation Algorithm
  - [ ] Backpropagation Intuition
  - [ ] Implementation Note: Unrolling Parameter
  - [ ] Gradient Checking
  - [ ] Random Initialization
  - [ ] Putting It Together
  - [ ] Autonomous Driving
  
  > [1 practice exercise]
  
  ### [Week 6](https://github.com/PryskaS/Machine-Learning_Stanford-University/tree/master/06.%20Week)
----------------------------------------------------------
- **Advice for Applying Machine Learning**
  - [ ] Deciding What to Try Next
  - [ ] Evaluating a Hypothesis
  - [ ] Model Selection and Train/Validation/Test Sets
  - [ ] Diagnosing Bias vs. Variance
  - [ ] Regularization and Bias/Variance
  - [ ] Learning Curves
  - [ ] Deciding What to Do Next Revisited
  
  > [1 practice exercise]
  
- **Machine Learning System Design**
  - [ ] Prioritizing What to Work On
  - [ ] Error Analysis
  - [ ] Error Metrics for Skewed Classes
  - [ ] Trading Off Precision and Recall
  - [ ] Data For Machine Learning
  
  > [1 practice exercise]
  
  ### [Week 7](https://github.com/PryskaS/Machine-Learning_Stanford-University/tree/master/07.%20Week)
----------------------------------------------------------
- **Support Vector Machines**
  - [ ] Optimization Objective
  - [ ] Large Margin Intuition
  - [ ] Mathematics Behind Large Margin Classification
  - [ ] Kernels I
  - [ ] Kernels II
  - [ ] Using An SVM
  
  > [1 practice exercise]
  
  ### [Week 8](https://github.com/PryskaS/Machine-Learning_Stanford-University/tree/master/08.%20Week)
----------------------------------------------------------
- **Unsupervised Learning**
  - [ ] Unsupervised Learning: Introduction
  - [ ] K-Means Algorithm
  - [ ] Optimization Objective
  - [ ] Random Initialization
  - [ ] Choosing the Number of Clusters
  
  > [1 practice exercise]
  
- **Dimensionality Reduction**
  - [ ] Motivation I: Data Compression
  - [ ] Motivation II: Visualization
  - [ ] Principal Component Analysis Problem Formulation
  - [ ] Principal Component Analysis Algorithm
  - [ ] Reconstruction from Compressed Representation
  - [ ] Choosing the Number of Principal Components
  - [ ] Advice for Applying PCA
  
  > [1 practice exercise]
  
  ### [Week 9](https://github.com/PryskaS/Machine-Learning_Stanford-University/tree/master/09.%20Week)
----------------------------------------------------------
- **Anomaly Detection**
  - [ ] Problem Motivation
  - [ ] Gaussian Distribution
  - [ ] Algorithm
  - [ ] Developing and Evaluating an Anomaly Detection System
  - [ ] Anomaly Detection vs. Supervised Learning
  - [ ] Choosing What Features to Use
  - [ ] Multivariate Gaussian Distribution
  - [ ] Anomaly Detection using the Multivariate Gaussian Distribution
  
  > [1 practice exercise]
  
- **Recommender Systems**
  - [ ] Problem Formulation
  - [ ] Content Based Recommendations
  - [ ] Collaborative Filtering
  - [ ] Collaborative Filtering Algorithm
  - [ ] Vectorization: Low Rank Matrix Factorization
  - [ ] Implementational Detail: Mean Normalization
  
  > [1 practice exercise]
  
  ### [Week 10](https://github.com/PryskaS/Machine-Learning_Stanford-University/tree/master/10.%20Week)
----------------------------------------------------------
- **Large Scale Machine Learning**
  - [ ] Learning With Large Datasets
  - [ ] Stochastic Gradient Descent
  - [ ] Mini-Batch Gradient Descent
  - [ ] Stochastic Gradient Descent Convergence
  - [ ] Online Learning
  - [ ] Map Reduce and Data Parallelism

  > [1 practice exercise]
  
  
  ### [Week 11](https://github.com/PryskaS/Machine-Learning_Stanford-University/tree/master/11.%20Week)
----------------------------------------------------------
- **Application Example: Photo OCR**
  - [ ] Problem Description and Pipeline
  - [ ] Sliding Windows
  - [ ] Getting Lots of Data and Artificial Data
  - [ ] Ceiling Analysis: What Part of the Pipeline to Work on Next
  - [ ] Summary and Thank You
  
  > [1 practice exercise]
  
 ReferÃªncias
---------------------------------------------------------------------
  
